# Toxic_comments_classifier

#Introduction: <br/>
To Filter out some online hate which occurs in our daily life due to some toxic people's anoymity.<br/>

#Objective: <br/>
Build a multi-headed model capable of detecting and assigning probabilities for different types of of toxicity such as: <br/>
1) toxic<br/>
2) severe toxic<br/>
3) obscene<br/>
4) threat<br/>
5) insult<br/>
6) identity hate<br/>
The dataset comprises of comments from Wikipedia's talk page (ie. discussion) pages.<br/>

#Installation:<br/>
1) Clone the repo to your local directory <br/>
2) Create a new environment through Anaconda Prompt <br/>
3) Launch the Jupyter Notebook and direct it to the path where you cloned the repo<br/>

#Conclusion: <br/>
Identify all the frequent words used in all the six categories of the comments.<br/>

Dataset: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge <br/>
Medium Blog: https://medium.com/@saurabh11iiitu/an-eda-approach-to-build-a-toxic-comments-classifier-56362aa017dd
